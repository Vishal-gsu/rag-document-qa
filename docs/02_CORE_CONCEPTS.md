# üìñ **02 - Core Concepts Explained**

## **1. What is RAG?**

**RAG = Retrieval-Augmented Generation**

It's a 2-step process:
```
Step 1: RETRIEVAL  - Find relevant info from documents
Step 2: GENERATION - Use that info to generate answer
```

### **Without RAG (LLM Only)**
```python
Q: "Who is the author of textbook A?"
LLM: "The author is Stephen... Marsland? Maybe?"
     (No source, might be wrong)
```

### **With RAG**
```python
Q: "Who is the author of textbook A?"
   ‚Üì
RETRIEVE: Search documents, find:
   "STEPHEN MARSLAND is the author"
   ‚Üì
GENERATE: Use Ollama to write:
   "According to the textbook, Stephen Marsland is the author"
   (Confident + sourced!)
```

---

## **2. Vector Databases & Similarity Search**

### **What are Vectors?**

Text ‚Üí Numbers (embeddings)

```
"Machine learning is AI" 
  ‚Üì (embedding model)
[0.23, 0.45, -0.12, 0.89, ..., -0.34]
  ‚Üì 384 dimensions
```

**Why numbers?**
- Computers understand numbers, not text
- Numbers capture MEANING
- Semantically similar texts have similar vectors

### **How Similarity Search Works**

```
Your Question: "What is machine learning?"
  ‚Üì
Embedding: [0.21, 0.47, -0.11, 0.88, ...]

Compare to stored vectors:
Chunk 1: [0.22, 0.46, -0.10, 0.89, ...] ‚Üí 98% similar ‚úÖ
Chunk 2: [0.15, 0.52, -0.08, 0.85, ...] ‚Üí 85% similar ‚úÖ
Chunk 3: [0.45, 0.12, 0.67, 0.23, ...] ‚Üí 15% similar ‚ùå

Return: Chunks 1 & 2 (high similarity)
```

---

## **3. HNSW Algorithm (Why it's Fast)**

### **The Problem: Linear Search is SLOW**

With 4,772 vectors:
```
‚ùå Brute Force: Compare query to ALL 4,772 vectors
   Time: ~100ms per query
   
‚úÖ HNSW: Navigate smart graph structure  
   Time: ~5ms per query
   Speedup: 20x faster!
```

### **How HNSW Works**

HNSW = **Hierarchical Navigable Small World**

**Hierarchical = Multiple Layers**
```
Layer 2:  O----O----O  (few vectors, long jumps)
Layer 1:  O--O--O--O  (more vectors, medium jumps)  
Layer 0:  O-O-O-O-O-O-O-O (all vectors, fine search)

Starting at top, navigate down layers to find nearest neighbors!
```

### **Search Process**

```
Query Vector: [0.21, 0.47, -0.11, ...]

1. Start at layer 2 (top):
   Find closest vector ‚Üí O

2. Go to layer 1:
   From that O, find closer neighbor ‚Üí O'

3. Go to layer 0 (bottom):
   From O', find all nearest neighbors
   
Final: Top 5 closest vectors found!
```

**Result: Only ~14 distance calculations instead of 4,772!**

---

## **4. Embeddings Explained**

### **What is an Embedding?**

Convert text to vector (fixed-size array of numbers)

```
Input: "Machine learning uses algorithms"
Model: sentence-transformers/all-MiniLM-L6-v2
Output: 384 numbers: [-0.012, 0.456, ..., 0.234]
```

### **Why 384 Dimensions?**

- **More = Better** (captures more meaning)
- **Less = Faster** (less computation)
- **384 = Sweet spot** (balance accuracy & speed)

```
64D:   Super fast, but loses info ‚ùå
384D:  Fast + accurate ‚úÖ (we use this)
1536D: Accurate but slower ‚ö†Ô∏è
```

### **Local vs Cloud Embeddings**

| Type | Model | Speed | Cost | Privacy |
|------|-------|-------|------|---------|
| **Local** | all-MiniLM-L6-v2 | Fast ‚úÖ | Free ‚úÖ | Private ‚úÖ |
| **Cloud** | OpenAI | Slower ‚ö†Ô∏è | $$ ‚ö†Ô∏è | Shared ‚ö†Ô∏è |

**We use LOCAL** ‚Üí No API costs, privacy, instant

---

## **5. Chunking Strategy**

### **Why Chunking?**

Documents are too long! Need to split them:

```
Entire PDF: 100 pages, 50,000 characters
  ‚Üì
Too big for embedding model!
  ‚Üì
Solution: Split into 1000-char chunks
  ‚Üì
Result: ~50 manageable chunks
```

### **Current Chunking Settings**

```python
CHUNK_SIZE = 1000      # Characters per chunk
CHUNK_OVERLAP = 200    # Overlap between chunks

Example:
Chunk 1: "...learning is the process of..."
Chunk 2: "...of discovering patterns in..."
         ^^^^^^^^ (200 char overlap)
```

**Why overlap?**
- If answer spans chunk boundary, overlap captures it
- Prevents losing information at edges

---

## **6. Prompt Engineering**

### **What is a Prompt?**

Instructions to the LLM on how to behave

```
Bad Prompt:
"Answer the question"
‚Üí LLM gets confused, makes up stuff

Good Prompt:
"Using ONLY the provided context, answer this question.
Be concise. Do not add follow-up questions."
‚Üí LLM follows rules, gives accurate answer
```

### **Our Prompt Optimization Journey**

**Before (Hallucinations):**
```
"I cannot confirm Stephen Marsland is the author..."
(Model too cautious, hedges even when answer clear)
```

**After (Direct):**
```
"Stephen Marsland is the author of this textbook."
(Model confident when evidence is clear)
```

**Key Changes:**
- ‚úÖ Removed: "I cannot confirm..."
- ‚úÖ Added: "Answer directly and confidently"
- ‚úÖ Set: Temperature=0.4 (focused, not creative)
- ‚úÖ Limited: max_tokens=800 (prevents rambling)

---

## **7. Similarity Threshold**

### **Why Filter by Threshold?**

Not all retrieved chunks are relevant!

```
Query: "What is machine learning?"
Chunk 1: 70% similar ‚úÖ (Keep)
Chunk 2: 65% similar ‚úÖ (Keep)
Chunk 3: 40% similar ‚úÖ (Keep)
Chunk 4: 25% similar ‚ùå (Discard - noise)
Chunk 5: 15% similar ‚ùå (Discard - irrelevant)

Threshold = 30%
Result: Use chunks 1-3 only
```

**Current Setting: 30% threshold**
- Filters out noise
- Prevents hallucinations from irrelevant chunks
- Still retrieves 3-5 relevant chunks

---

## **8. Temperature Parameter**

Controls how "creative" vs "focused" the LLM is:

```
Temperature = 0.0 (Deterministic)
‚Üí Always same answer, very rigid
‚Üí Use when: Accuracy critical

Temperature = 0.4 (Balanced) ‚úÖ (We use this)
‚Üí Focused but natural
‚Üí Use when: RAG, factual answers

Temperature = 1.0 (Creative)
‚Üí Varies answer, more creative
‚Üí Use when: Story writing, brainstorming
```

---

## **9. Tokens**

### **What is a Token?**

A piece of text (roughly 0.75 words per token)

```
"Machine learning" = 2 tokens
"is a subset of artificial intelligence" = 7 tokens

Total sentence ‚âà 9 tokens
```

### **Token Limits**

```
max_tokens=150:  ~110 words (too short)
max_tokens=800:  ~600 words (our default)
max_tokens=1500: ~1125 words (very detailed)
```

**Why limit?**
- Prevent rambling
- Limit inference cost
- Generate in reasonable time

---

## **10. Docker & Persistence**

### **What is Docker?**

Package software + dependencies in a container

```
Endee Server:
  Before: "How do I install Endee?"
  After: "docker compose up -d" ‚úÖ

Benefits:
- Same on any computer ‚úÖ
- No dependency conflicts ‚úÖ
- Data persists ‚úÖ
```

### **Our Docker Setup**

```yaml
services:
  endee:
    image: endeeio/endee-server:latest
    ports:
      - "8080:8080"
    volumes:
      - endee-data:/data  # Persistent storage!
```

**Key Point:** `endee-data` volume persists across restarts!

---

## **11. LLM Options**

### **3 Different LLM Providers**

#### **1. OpenAI API** ‚òÅÔ∏è
```
Pros:
- Best quality (GPT-3.5-turbo)
- No hardware needed
- Cloud-based

Cons:
- Costs $$
- Requires API key
- Shared servers
```

#### **2. Ollama GPU** üöÄ
```
Pros:
- Free (open source)
- Fast (GPU accelerated)
- Privacy (local)

Cons:
- Needs GPU
- Slower than OpenAI
```

#### **3. Ollama CPU** üíª
```
Pros:
- Free
- Privacy
- No GPU needed

Cons:
- Slow (~2 tokens/sec vs 20 with GPU)
- High CPU usage
```

**We use Ollama GPU** (phi3 model, free, fast, private)

---

## **12. Evaluation Metrics**

### **How Do We Measure Success?**

```
Retrieval Quality:
- Similarity scores: 60-70% ‚úÖ
- Relevant chunks retrieved ‚úÖ

Generation Quality:
- No hallucinations ‚úÖ
- Sources are accurate ‚úÖ
- Answers are concise ‚úÖ

Performance:
- Query time: 2-3 seconds ‚úÖ
- Search time: <10ms ‚úÖ
```

---

## **Key Formulas**

### **Cosine Similarity**
```
Given two vectors A and B:

Similarity = (A ¬∑ B) / (||A|| √ó ||B||)

Range: 0 (completely different) to 1 (identical)
```

### **HNSW Complexity**
```
Brute Force Search:  O(n) = comparing all n vectors
HNSW Search:         O(log n) = logarithmic!

Example with 4,772 vectors:
Brute: 4,772 comparisons
HNSW: ~log(4,772) ‚âà 12 comparisons ‚Üê 400x faster!
```

---

## **Quick Recap**

| Concept | What | Why |
|---------|------|-----|
| **RAG** | Retrieve + Generate | Prevent hallucination |
| **Vectors** | Numbers from text | Enable similarity search |
| **HNSW** | Smart graph search | 100-1000x faster |
| **Embeddings** | Text ‚Üí Numbers | Semantic understanding |
| **Chunking** | Split documents | Manageable size |
| **Prompting** | Instructions to LLM | Control behavior |
| **Threshold** | Similarity cutoff | Filter noise |
| **Temperature** | Creativity level | Balance accuracy/variety |
| **Tokens** | Text pieces | Count response length |
| **Docker** | Container | Reproducibility |

---

## **Next: Deep Math** üìö

Ready for the hardcore theory? Go to **THEORY.md** for:
- Vector mathematics
- HNSW algorithm details
- Information retrieval formulas
- Transformer embedding models

**Or skip to code:** Go to **04_CODE_WALKTHROUGH.md** to see it all in action!
