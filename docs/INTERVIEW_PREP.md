# Interview Preparation Guide

This guide will help you prepare for technical interviews about your RAG project.

---

## Table of Contents
1. [Conceptual Questions](#conceptual-questions)
2. [Technical Implementation](#technical-implementation)
3. [Code Walkthrough](#code-walkthrough)
4. [System Design](#system-design)
5. [Troubleshooting & Debugging](#troubleshooting--debugging)
6. [Future Improvements](#future-improvements)

---

## Conceptual Questions

### Q1: What is RAG and why is it important?

**Answer:**
RAG stands for Retrieval Augmented Generation. It's a technique that combines:
1. **Retrieval**: Fetching relevant information from a knowledge base
2. **Augmentation**: Adding retrieved context to the LLM prompt
3. **Generation**: Producing answers using the augmented prompt

**Why Important:**
- Reduces hallucination by grounding responses in actual documents
- Provides access to proprietary/recent information not in LLM training data
- Enables source citation and verification
- More cost-effective than fine-tuning for domain-specific knowledge

**Real-world Use Cases:**
- Customer support chatbots
- Internal documentation Q&A
- Research paper analysis
- Legal document review

---

### Q2: Explain vector embeddings in simple terms

**Answer:**
Vector embeddings convert text into numbers (vectors) that capture meaning.

**Analogy:**
Think of it like a GPS coordinate system. Words with similar meanings are "close together" in this multi-dimensional space.

**Example:**
```
"dog" ‚Üí [0.2, 0.8, -0.3, ...]
"puppy" ‚Üí [0.25, 0.75, -0.28, ...] ‚Üê Close to "dog"
"car" ‚Üí [-0.6, 0.1, 0.9, ...] ‚Üê Far from "dog"
```

**Key Properties:**
- Semantic similarity ‚Üí Vector similarity
- Typically 384-3072 dimensions
- Generated by neural networks trained on large text corpora

---

### Q3: How does similarity search work?

**Answer:**
1. **Convert query to vector** using same embedding model
2. **Calculate distance** between query vector and all stored vectors
3. **Rank by similarity** using metrics like cosine similarity
4. **Return top-K** most similar results

**Cosine Similarity:**
- Measures angle between vectors
- Range: -1 (opposite) to 1 (identical)
- Formula: `cos(Œ∏) = (A¬∑B) / (||A|| √ó ||B||)`

**Why it works:**
Vectors encode semantic meaning, so similar meanings = similar vectors = high similarity scores

---

### Q4: What problems does RAG solve that standard LLMs don't?

**Answer:**

| Problem | Standard LLM | RAG System |
|---------|-------------|------------|
| Knowledge cutoff | ‚ùå Can't access recent info | ‚úÖ Updated with new docs |
| Hallucination | ‚ùå May fabricate facts | ‚úÖ Grounded in sources |
| Domain specificity | ‚ùå General knowledge only | ‚úÖ Custom knowledge base |
| Source citation | ‚ùå Can't cite sources | ‚úÖ Returns source docs |
| Cost | ‚ùå Expensive fine-tuning | ‚úÖ Update docs, not model |

---

## Technical Implementation

### Q5: Walk me through your indexing pipeline

**Answer:**

```
1. DOCUMENT LOADING
   - Scan directory for supported files (.txt, .md, .pdf, .docx)
   - Read content using appropriate parsers
   - Extract metadata (filename, type, source)

2. CHUNKING
   - Split large documents into manageable pieces (500 chars)
   - Use overlap (50 chars) to preserve context across boundaries
   - Try to break at sentence boundaries for coherence

3. EMBEDDING GENERATION
   - Send chunks to OpenAI embedding API in batches
   - Receive 1536-dimensional vectors
   - Batch size: 100 to optimize API calls

4. STORAGE
   - Save vectors with metadata to Endee Vector Database
   - Persist to disk for future queries
   - Create indexes for fast similarity search
```

**Code reference:** `rag_engine.py::index_documents()`

---

### Q6: How does your query pipeline work?

**Answer:**

```
1. QUERY EMBEDDING (200-500ms)
   - Convert user question to vector
   - Use same embedding model as indexing
   
2. SIMILARITY SEARCH (10-50ms)
   - Calculate cosine similarity with all stored vectors
   - Sort by similarity score
   - Return top-3 most relevant chunks

3. CONTEXT BUILDING
   - Extract text from retrieved chunks
   - Format with source information
   - Build structured prompt

4. LLM GENERATION (1-3 seconds)
   - Send prompt with context to GPT-3.5/4
   - Receive generated answer
   - Return to user

Total latency: ~1.5-4 seconds
```

**Code reference:** `rag_engine.py::query()`

---

### Q7: Why did you choose your chunking strategy?

**Answer:**

**Chosen Strategy:**
- Chunk size: 500 characters
- Overlap: 50 characters
- Sentence-aware splitting

**Reasoning:**

**Size (500 chars):**
- ‚úÖ Small enough for focused retrieval
- ‚úÖ Large enough to preserve context
- ‚úÖ Balances precision vs recall
- ‚ùå Too small (<200): Fragmented context
- ‚ùå Too large (>1000): Less precise retrieval

**Overlap (50 chars):**
- ‚úÖ Prevents losing context at boundaries
- ‚úÖ Helps with queries that span chunk boundaries
- ‚ùå Trade-off: More storage and computation

**Sentence-aware:**
- ‚úÖ Breaks at natural boundaries (periods, newlines)
- ‚úÖ More coherent chunks
- ‚úÖ Better for LLM comprehension

**Alternative considered:**
- Semantic chunking (split by topic) - more complex but potentially better
- Fixed-token chunking - consistent token count but may break mid-sentence

---

### Q8: How does your vector database work?

**Answer:**

**Current Implementation** (Simplified for demo):
```python
# Storage format
{
    'vectors': List[List[float]],  # Embeddings
    'metadata': List[Dict]          # Text + metadata
}
# Persisted as: data/vectordb/collection.pkl
```

**Operations:**
1. **Add**: Append vectors and metadata, save to disk
2. **Search**: Iterate all vectors, compute similarity, sort, return top-K
3. **Load**: Read from disk on startup

**Production Considerations:**
- Current: O(n) search - fine for small datasets
- Production: Use approximate nearest neighbor (ANN) indexes
  - HNSW: Graph-based, fast queries
  - IVF: Cluster-based, good for large datasets
  - LSH: Hash-based, memory efficient

**Why Endee:**
- Simple API
- Good for prototyping
- Local-first (no cloud dependency)
- Easy to understand and debug

---

## Code Walkthrough

### Q9: Explain this code snippet from your project

**Interviewer might show:**
```python
def _split_text(self, text: str) -> List[str]:
    if len(text) <= self.chunk_size:
        return [text]
    
    chunks = []
    start = 0
    
    while start < len(text):
        end = start + self.chunk_size
        
        if end < len(text):
            boundary = max(
                text.rfind('.', start, end),
                text.rfind('?', start, end),
                text.rfind('!', start, end),
                text.rfind('\n', start, end)
            )
            
            if boundary > start:
                end = boundary + 1
        
        chunk = text[start:end].strip()
        if chunk:
            chunks.append(chunk)
        
        start = end - self.chunk_overlap
    
    return chunks
```

**Answer:**

**Purpose:** Split text into overlapping chunks while respecting sentence boundaries

**Flow:**
1. **Base case**: If text ‚â§ chunk_size, return as-is
2. **Initialize**: Start at position 0
3. **Loop** through text:
   - Calculate tentative end position
   - Look for sentence endings (., ?, !, \n) before end
   - Use latest boundary if found (preserves sentences)
   - Extract chunk and add to list
   - Move start forward with overlap
4. **Return** list of chunks

**Key Details:**
- `rfind()`: Find rightmost occurrence (closest to end)
- `max()`: Choose latest boundary among all punctuation
- `boundary + 1`: Include the punctuation mark
- `strip()`: Remove leading/trailing whitespace
- `start = end - overlap`: Creates overlap between chunks

**Why this approach:**
- Preserves sentence completeness
- Maintains context across chunks
- Gracefully handles edge cases (no boundaries found)

---

### Q10: What would you change if this needed to handle 1M documents?

**Answer:**

**Current Bottlenecks:**
1. In-memory vector storage
2. Linear search O(n)
3. Sequential embedding generation
4. Single-threaded processing

**Solutions:**

**1. Vector Database**
```python
# Replace pickle storage with production vector DB
from pinecone import Pinecone  # or Weaviate, Milvus, etc.

pc = Pinecone(api_key="...")
index = pc.Index("documents")

# Approximate nearest neighbor (ANN) search
# Reduces O(n) to O(log n) or better
```

**2. Distributed Embedding**
```python
# Process in parallel
from concurrent.futures import ThreadPoolExecutor

with ThreadPoolExecutor(max_workers=10) as executor:
    embeddings = list(executor.map(embed_batch, batches))
```

**3. Incremental Indexing**
```python
# Only embed new/changed documents
def index_new_documents(since_timestamp):
    new_docs = get_docs_after(since_timestamp)
    # Index only new_docs, not all
```

**4. Caching**
```python
# Cache frequent queries
from functools import lru_cache

@lru_cache(maxsize=1000)
def cached_query(question: str):
    return query(question)
```

**5. Async Processing**
```python
import asyncio

async def async_embed(texts):
    # Non-blocking API calls
    return await openai.embeddings.create_async(...)
```

**Architecture Changes:**
- Message queue (RabbitMQ, Kafka) for embedding jobs
- Distributed storage (S3, MinIO)
- Load balancer for query distribution
- Monitoring (Prometheus, Grafana)

---

## System Design

### Q11: How would you design this for production use?

**Answer:**

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                     PRODUCTION RAG SYSTEM                ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                          ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                  ‚îÇ
‚îÇ  ‚îÇ  Web UI    ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ   API      ‚îÇ                  ‚îÇ
‚îÇ  ‚îÇ (React)    ‚îÇ         ‚îÇ (FastAPI)  ‚îÇ                  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                  ‚îÇ
‚îÇ                               ‚îÇ                          ‚îÇ
‚îÇ                               ‚ñº                          ‚îÇ
‚îÇ                        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                    ‚îÇ
‚îÇ                        ‚îÇ   Auth     ‚îÇ                    ‚îÇ
‚îÇ                        ‚îÇ  Service   ‚îÇ                    ‚îÇ
‚îÇ                        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                    ‚îÇ
‚îÇ                              ‚îÇ                           ‚îÇ
‚îÇ              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê          ‚îÇ
‚îÇ              ‚ñº                               ‚ñº          ‚îÇ
‚îÇ       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îÇ
‚îÇ       ‚îÇ  Query      ‚îÇ              ‚îÇ  Indexing    ‚îÇ     ‚îÇ
‚îÇ       ‚îÇ  Service    ‚îÇ              ‚îÇ  Service     ‚îÇ     ‚îÇ
‚îÇ       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ
‚îÇ              ‚îÇ                            ‚îÇ             ‚îÇ
‚îÇ              ‚ñº                            ‚ñº             ‚îÇ
‚îÇ       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îÇ
‚îÇ       ‚îÇ Vector DB      ‚îÇ          ‚îÇ Document    ‚îÇ       ‚îÇ
‚îÇ       ‚îÇ (Pinecone)     ‚îÇ          ‚îÇ Store (S3)  ‚îÇ       ‚îÇ
‚îÇ       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îÇ
‚îÇ              ‚îÇ                                          ‚îÇ
‚îÇ              ‚ñº                                          ‚îÇ
‚îÇ       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                ‚îÇ
‚îÇ       ‚îÇ  LLM Service   ‚îÇ                                ‚îÇ
‚îÇ       ‚îÇ  (OpenAI/GPT)  ‚îÇ                                ‚îÇ
‚îÇ       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  Monitoring: Prometheus + Grafana                       ‚îÇ
‚îÇ  Logging: ELK Stack                                     ‚îÇ
‚îÇ  Caching: Redis                                         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Components:**

1. **API Layer** (FastAPI)
   - RESTful endpoints
   - Request validation
   - Rate limiting

2. **Authentication** (Auth0, JWT)
   - User management
   - API key auth
   - Role-based access

3. **Query Service**
   - Handle search requests
   - Manage context building
   - LLM interaction

4. **Indexing Service**
   - Background job processing
   - Document ingestion
   - Embedding generation

5. **Storage**
   - Vector DB: Pinecone/Weaviate
   - Document storage: S3
   - Metadata: PostgreSQL

6. **Monitoring**
   - Metrics: Query latency, error rates
   - Logging: Structured logs
   - Alerts: Anomaly detection

7. **Caching** (Redis)
   - Query results
   - Embeddings
   - Hot documents

---

## Troubleshooting & Debugging

### Q12: How would you debug poor retrieval quality?

**Answer:**

**Symptoms:**
- Irrelevant chunks retrieved
- Missing relevant information
- Low similarity scores

**Debugging Steps:**

**1. Inspect Retrieved Chunks**
```python
results = vector_store.search(query_vector, top_k=10)
for i, result in enumerate(results):
    print(f"{i}. Score: {result['score']:.4f}")
    print(f"   Text: {result['text'][:200]}")
    print(f"   Source: {result['metadata']['source']}")
```

**2. Check Similarity Scores**
- Good: > 0.7
- Acceptable: 0.5-0.7
- Poor: < 0.5

**3. Verify Embedding Quality**
```python
# Test embedding consistency
text = "machine learning"
emb1 = embed_text(text)
emb2 = embed_text(text)
print(cosine_similarity(emb1, emb2))  # Should be ~1.0
```

**4. Analyze Chunking**
```python
# Are chunks too large/small?
chunk_sizes = [len(chunk['text']) for chunk in chunks]
print(f"Avg: {np.mean(chunk_sizes)}")
print(f"Min: {min(chunk_sizes)}, Max: {max(chunk_sizes)}")
```

**5. Test Query Variations**
```python
queries = [
    "What is ML?",
    "Explain machine learning",
    "Machine learning definition"
]
# See which formulation retrieves better results
```

**Solutions:**

- **Adjust chunk size**: Try 300, 500, 800 characters
- **Increase K**: Retrieve more chunks (5-10 instead of 3)
- **Re-rank**: Use cross-encoder for better ranking
- **Query expansion**: Add synonyms, rephrase
- **Hybrid search**: Combine vector + keyword search

---

## Future Improvements

### Q13: What features would you add next?

**Answer:**

**Priority 1: Core Improvements**
1. **Conversation Memory**
   - Track chat history
   - Resolve pronoun references
   - Multi-turn conversations

2. **Re-ranking**
   - Use cross-encoder after initial retrieval
   - Improves relevance of top results
   - Trade-off: Slight latency increase

3. **Hybrid Search**
   - Combine vector (semantic) + BM25 (keyword)
   - Better for specific terms/names
   - Reciprocal Rank Fusion to combine scores

**Priority 2: User Experience**
4. **Web UI** (Streamlit/Gradio)
   ```python
   import streamlit as st
   
   question = st.text_input("Ask a question:")
   if question:
       answer = rag.query(question)
       st.write(answer)
   ```

5. **Streaming Responses**
   - Show answer as it's generated
   - Better perceived performance
   ```python
   for chunk in openai.chat.completions.create(stream=True):
       print(chunk.choices[0].delta.content, end="")
   ```

6. **Source Citations**
   - Highlight which chunks were used
   - Link to original documents
   - Confidence scores

**Priority 3: Advanced Features**
7. **Multi-modal Support**
   - Images in PDFs
   - Tables and charts
   - Audio/video transcripts

8. **Query Analytics**
   - Track popular questions
   - Identify knowledge gaps
   - A/B testing

9. **Automated Evaluation**
   - RAGAS metrics
   - Regression testing
   - Quality monitoring

**Example: Adding Conversation Memory**
```python
class ConversationalRAG(RAGEngine):
    def __init__(self):
        super().__init__()
        self.history = []
    
    def query_conversational(self, question):
        # Reformulate question with history context
        full_context = self._build_context_with_history(question)
        answer = self.query(full_context)
        self.history.append((question, answer))
        return answer
```

---

## Demonstrating Your Knowledge

### Tips for the Interview

**1. Start High-Level, Go Deep**
```
"RAG combines retrieval and generation..."
[If asked]: "The retrieval uses cosine similarity on embeddings..."
[If asked]: "Cosine similarity is calculated as dot product divided by..."
```

**2. Use Analogies**
- Embeddings = GPS coordinates for meaning
- Vector DB = Librarian that finds similar books
- RAG = Looking up facts before writing an essay

**3. Show Trade-offs**
```
"I chose chunk size 500 because:
‚úÖ Pros: Balances context and precision
‚ùå Cons: May split long explanations
Alternative: Semantic chunking (more complex)"
```

**4. Relate to Real World**
```
"This is similar to how Google uses embeddings for search..."
"Amazon recommendations use similar vector similarity..."
```

**5. Be Honest About Limitations**
```
"My current implementation uses O(n) search which won't scale.
In production, I'd use HNSW indexes for O(log n) performance."
```

---

## Practice Questions

**Before your interview, practice explaining:**

‚úÖ RAG vs standard LLM vs fine-tuning  
‚úÖ Your indexing pipeline step-by-step  
‚úÖ How similarity search works mathematically  
‚úÖ Trade-offs in your design decisions  
‚úÖ How you'd scale to 1M documents  
‚úÖ How you'd debug poor retrieval  
‚úÖ One feature you'd add and why  

---

## Final Advice

**Do:**
- ‚úÖ Draw diagrams while explaining
- ‚úÖ Use concrete examples from your code
- ‚úÖ Explain your thought process
- ‚úÖ Ask clarifying questions
- ‚úÖ Admit what you don't know

**Don't:**
- ‚ùå Memorize explanations robotically
- ‚ùå Get lost in implementation details initially
- ‚ùå Ignore trade-offs in your decisions
- ‚ùå Claim expertise beyond your actual knowledge
- ‚ùå Forget to relate back to business value

**Remember:** The goal is to demonstrate:
1. **Understanding**: You know how RAG works
2. **Implementation**: You can build it
3. **Critical Thinking**: You understand trade-offs
4. **Growth Mindset**: You know what to improve

---

Good luck with your interview! üöÄ
